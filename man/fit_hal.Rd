% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/formula_hal9001.R, R/hal.R
\name{formula_hal}
\alias{formula_hal}
\alias{fit_hal}
\alias{fit_hal.formula_hal9001}
\alias{fit_hal.default}
\title{HAL: The Highly Adaptive Lasso}
\usage{
formula_hal(
  formula,
  data,
  smoothness_orders = NULL,
  num_knots = NULL,
  exclusive_dot = FALSE,
  custom_group = NULL,
  adaptive_smoothing = FALSE,
  ...
)

fit_hal(X, ...)

\method{fit_hal}{formula_hal9001}(X, ...)

\method{fit_hal}{default}(
  X,
  Y,
  X_unpenalized = NULL,
  max_degree = ifelse(ncol(X) >= 20, 2, 3),
  smoothness_orders = rep(1, ncol(X)),
  num_knots = sapply(seq_len(max_degree), num_knots_generator, smoothness_orders =
    smoothness_orders, base_num_knots_0 = 500, base_num_knots_1 = 200),
  fit_type = c("glmnet", "lassi"),
  n_folds = 10,
  foldid = NULL,
  use_min = TRUE,
  reduce_basis = NULL,
  family = c("gaussian", "binomial", "poisson", "cox"),
  return_lasso = TRUE,
  return_x_basis = FALSE,
  basis_list = NULL,
  lambda = NULL,
  id = NULL,
  offset = NULL,
  cv_select = TRUE,
  adaptive_smoothing = FALSE,
  prediction_bounds = "default",
  ...,
  lambda.min.ratio = 1e-04,
  yolo = FALSE
)
}
\arguments{
\item{formula}{A character string specifying the hal9001 model.
The format should be of the form "y ~ h(x) + h(w,x) + h(x,w) + h(x,w,z) "
where "y" is the outcome and "w,x,y,z" are variables in \code{data}.
Each term represents a main-term/interaction to be included in the model.
h(x) signifies that all
one-way/main term basis functions of the variable x should be included.
h(x,w) specifies that all interaction (two-way) basis functions between x and w
should be included in the model. Similarly, h(x,w,z) specifies
that all interaction (three-way) basis functions between x,w,z should be included in the model.
Note that "y ~ h(x,y,z)" will only construct three-way basis functions for x,y,z
and not the two-way and one-way basis functions.
Additionally, a formula of the form "y ~ ." will generate all one-way main term basis functions for variables in \code{data}.
Similarly, "y ~ .^2" will generate all basis functions up to degree 2 for all variables in \code{data}.
More generally, "y ~ .^max_degree" will construct all basis functions up to degree max_degree.
One can combine all the notions above. For example,
"y ~ h(x,w,z) + ." and "y ~ h(x,w,z) + .^2" will generate all one-way (resp. up to two-way) basis functions
and additionally all the three-way interaction basis functions between variables w,x,z.
One can also specify monotonicity constraints by replacing the letter \code{h} with \code{d} (for decreasing) or \code{i} (for increasing), e.g. formulas like "y ~ i(x) + i(y) + i(x,y)", "y ~ d(x) + d(y) + d(x,y)", or "y ~ d(x) + i(y) + h(x,y)".
The letters h, i, d specify functional restrictions of each term:
h specifies no constraints on the term,
i specifies that the term should be enforced to be monotonely increasing,
d specifies that the term should be enforced to be monotonely decreasing.
Ambigious operations like "y ~ i(x) + ."  will use the first specification/evaluation of the term in the formula (generally from left to right).
That is, "y ~ i(x) + ." -> "y ~ i(x) + h(z) + h(w)" and "y ~ h(x) + i(x)" -> "y ~ h(x)".
Note that "." and ".^max_degree" have the lowest importance and are evaluated last, regardless of their location in the formula.
As a result, "y ~ . + i(x)" -> "y ~ i(x) + h(w) + h(z), contrary to the previous case.
Familar operations such as the ":", \code{*} ,"-" are also supported:
":" is a concatnation operator which maps h(x):h(w) -> h(x,w) or h(x):h(w):h(z) -> h(x,w,z)
\code{*} concatenates and then generates all lower order terms/interactions. For example, h(x)*h(w) -> h(x) + h(w) + h(x,w)
or h(x)\code{*}h(w)\code{*}h(z) -> h(x) + h(w) + h(z) + h(x,w) + h(x,z) + h(z,w) + h(x,w,z).
"-" subtracts/removes the term from the formula. For example, h(x) + h(w) - h(w) -> h(x).
Note that the above operations are sensitive to the constraint prefix "h, i, d".
For ambigious operations such as i(x):h(w), the unconstrained prefix "h" will be used unless all prefixes in the term are the same.
So i(x):h(w) -> h(x,w) and i(x):i(w):d(z) -> h(x,w,z) and i(x):i(w) -> i(x,w).
The above logic will be applied recursively to \code{*} so that i(x)\code{*}h(w)\code{*}i(z) -> i(x) + h(w) + i(z) + h(x,w) + i(x,z) + h(w,z) + h(x,w,z).
Another useful operation is the wild-card "." operation which when used in a specified term will generate
all valid terms where the value of "." is iterated over the non-outcome columns in the data matrix. For example,
h(x,.) -> h(x,w) + h(x,z) and h(.,.) -> h(x,w) + h(x,z) + h(w,z) and h(x,w,.) -> h(x,w,z) (assuming the covariates are only (x,w,z)).
All these operations are compatible with one another, e.g. h(.)*h(x), h(.):h(x)  and h(x) - h(.) are valid and behave as expected.}

\item{data}{A data.frame or named matrix containing the outcome and covariates specified in the formula.}

\item{smoothness_orders}{An \code{integer} vector of length 1 or length
\code{ncol(X)}. If \code{smoothness_orders} is of length 1, then its values
are recycled to form a vector of length length \code{ncol(X)}. Given such a
vector of length \code{ncol(X)}, the ith element specifies the level of
smoothness for the variable corresponding with the ith column in \code{X}.
A value of "0" corresponds with 0-order splines (piece-wise constant) which
assumes no smoothness or continuity of true regression function. A value of
"1" corresponds with 1-order splines (piece-wise linear) which only assumes
continuity of true regression function. A value of "2" corresponds with
2-order splines (piece-wise quadratic and linear terms) which assumes one
order of differentiability for the true regression function. WARNING: if
\code{smoothness_orders} has length less than \code{ncol(X)}, then values
are recycled as needed.}

\item{num_knots}{An \code{integer} vector of length 1 or length
\code{max_degree}. If \code{num_knots} is a vector of length 1 then its
values are recycled to produce a vector of length \code{max_degree}. Given
a possibly recycled vector of length \code{max_degree}, num_knots\link{i}
specifies the maximum number of knot points used when generating basis
functions of degree i for each covariate. For example, num_knots\link{1}
specifies how many knot points to use when generating main-term additive
basis functions. \code{num_knots[2]} specifies how many knot points should
be used when generating each univariate basis function in the 2-tensor
product basis functions. A smaller number of knot points gives rise to a
less smooth function. However, fewer knot points can significantly decrease
runtime. If smoothness_orders is 1 or higher then few knot points (10-30)
are needed to maintain near optimal performance. For smoothness_orders = 0,
too few knot points (< 50) can significantly reduce performance. We
recommend specifying a vector of length \code{max_degree} that decreases
exponentially to prevent combinatorical explosions in the number of higher
degree interaction basis functions generated. Default: For zero order
smoothness (any(\code{smoothness_orders}==0)), the number of knots by
interaction degree \code{d} decays as \code{500/2^{d-1}}. For first or higher order
smoothness (all(\code{smoothness_orders}>0)), the number of knots by
interaction degree \code{d} decays as \code{75/2^{d-1}}. These defaults ensure that
the number of basis functions and thus the complexity of the optimization
problem grows scalably in \code{max_degree}.
\itemize{
\item Some good settings for little to no cost in performance:
\itemize{
\item If smoothness_orders = 0, max_degree = 3, num_knots = c(400, 200, 100).
\item If smoothness_orders = 1+, max_degree = 3, num_knots = c(100, 75, 50).
}
\item Recommended settings for fairly fast runtime and great performance:
\itemize{
\item If smoothness_orders = 0, max_degree = 3, num_knots = c(200, 100, 50).
\item If smoothness_orders = 1+, max_degree = 3, num_knots = c(50, 25, 15).
}
\item Recommended settings for fast runtime and good/great performance:
\itemize{
\item If smoothness_orders = 0, max_degree = 3, num_knots = c(100, 50, 25).
\item If smoothness_orders = 1+, max_degree = 3, num_knots = c(40, 15, 10).
}
\item Recommended settings for very fast runtime and good performance:
\itemize{
\item If smoothness_orders = 0, max_degree = 3, num_knots = c(50, 25, 10).
\item If smoothness_orders = 1+, max_degree = 3, num_knots = c(25, 10, 5).
}
}}

\item{exclusive_dot}{Boolean indicator for whether the "." and ".^max_degree" arguments in the formula
should be treated as exclusive or inclusive the variables already specified in the formula.
For example, if "y ~ h(x,w) + ." should the "." be interpreted as: add all one-way basis functions
for the variables remaining in \code{data} not yet specified in the formula (i.e. excluding x,w),
or: add all one-way basis functions for all variables in the data (including x,w).
As an example. if \code{exclusive_dot} is false then "y ~ h(x) + .^2" and "y ~ .^2" specify the same formula, i.e. generate all basis functions up to degree 2.
However, if \code{exclusive_dot} is true, then "y ~ h(x) + .^2"  encodes a different formula than "y ~ .^2".
Specifically, it means to generate one way basis functions for 'x' and then all basis functions
up to degree 2 for other variables excluding 'x' in \code{data}. As a result, no interactions will be added for the variable 'x'.}

\item{custom_group}{A named list with single character names representing a group, and elements being a character vector of variable names.
This allows the user to specify their own wild card symbols (e.g. '.').
However, the value of the symbol will be iterated over all variables specified in the user supplied group.
For example, if custom_group = list("1" = c("x", "w"), "2" = c("t","r"))
then then the following formula is mapped as "y ~ h(1,2)" -> "y ~ h(x,t) + h(x,r) + h(w,t) + h(w,r)",
so that all two way interactions using one variable for each group are generated.
Similarly, "y ~ h(1,r)" -> "y ~ h(x,r) + h(w,r)".
Thus, the custom groups operate exactly as "." except the possible values are restricted to a specific group.}

\item{adaptive_smoothing}{A \code{logical}, which, if \code{TRUE}, HAL will
perform adaptive smoothing up until the maximum order of smoothness
specified by \code{smoothness_orders}. For example, if
\code{smoothness_orders = 2} and \code{adaptive_smoothing = TRUE}, then HAL
will generate all basis functions of smoothness order 0, 1, and 2, and
data-adaptively select the basis functions to use. WARNING: This can
increase runtime by a factor of 2-3+ depending on value of
\code{smoothness_orders}.}

\item{...}{Other arguments passed to \code{\link[glmnet]{cv.glmnet}}. Please
consult its documentation for a full list of options.}

\item{X}{An input \code{matrix} containing observations and covariates.}

\item{Y}{A \code{numeric} vector of obervations of the outcome variable.}

\item{X_unpenalized}{An input \code{matrix} with the same format as X, that
directly get appended into the design matrix (no basis expansion). No L1
penalization is performed on these covariates.}

\item{max_degree}{The highest order of interaction terms for which the basis
functions ought to be generated. The default (\code{NULL}) corresponds to
generating basis functions for the full dimensionality of the input matrix.}

\item{fit_type}{The specific routine to be called when fitting the Lasso
regression in a cross-validated manner. Choosing the \code{glmnet} option
will result in a call to \code{\link[glmnet]{cv.glmnet}} while \code{lassi}
will produce a (faster) call to a custom Lasso routine.}

\item{n_folds}{Integer for the number of folds to be used when splitting the
data for V-fold cross-validation. This defaults to 10.}

\item{foldid}{An optional \code{numeric} containing values between 1 and
\code{n_folds}, identifying the fold to which each observation is assigned.
If supplied, \code{n_folds} can be missing. In such a case, this vector is
passed directly to \code{\link[glmnet]{cv.glmnet}}.}

\item{use_min}{Specify lambda selected by \code{\link[glmnet]{cv.glmnet}}.
\code{TRUE}, \code{"lambda.min"} is used; otherwise, \code{"lambda.1se"}.}

\item{reduce_basis}{A \code{numeric} value bounded in the open interval
(0,1) indicating the minimum proportion of 1's in a basis function column
needed for the basis function to be included in the procedure to fit the
Lasso. Any basis functions with a lower proportion of 1's than the cutoff
will be removed. This argument defaults to \code{NULL}, in which case all
basis functions are used in the lasso-fitting stage of the HAL algorithm.}

\item{family}{A \code{character} or a \code{\link[stats]{family}} object
(supported by \code{\link[glmnet]{glmnet}}) corresponding to the error/link
family for a generalized linear model. \code{character} options are limited
to "gaussian" for fitting a standard penalized linear model, "binomial" for
penalized logistic regression, "poisson" for penalized Poisson regression,
and "cox" for a penalized proportional hazards model. Note that in all
cases where family is not set to "gaussian", \code{fit_type} is limited to
"glmnet". NOTE: Passing in family objects lead to signficantly slower
performance relative to passing in a character family (if supported). Thus,
for nonparametric (HAL) logistic regression, one should always set
\code{family = "binomial"} and never set \code{family = binomial()}.}

\item{return_lasso}{A \code{logical} indicating whether or not to return
the \code{glmnet} fit of the lasso model.}

\item{return_x_basis}{A \code{logical} indicating whether or not to return
the matrix of (possibly reduced) basis functions used in the HAL lasso fit.}

\item{basis_list}{The full set of basis functions generated from the input
data X (via a call to \code{enumerate_basis}). The dimensionality of this
structure is dim = (n * 2^(d - 1)), where n is the number of observations
and d is the number of columns in X.}

\item{lambda}{User-specified array of values of the lambda tuning parameter
of the Lasso L1 regression. If \code{NULL}, \code{\link[glmnet]{cv.glmnet}}
will be used to automatically select a CV-optimal value of this
regularization parameter. If specified, the Lasso L1 regression model will
be fit via \code{glmnet}, returning regularized coefficient values for each
value in the input array.}

\item{id}{a vector of ID values, used to generate cross-validation folds for
cross-validated selection of the regularization parameter lambda.}

\item{offset}{a vector of offset values, used in fitting.}

\item{cv_select}{A \code{logical} specifying whether the array of values
specified should be passed to \code{\link[glmnet]{cv.glmnet}} in order to
pick the optimal value (based on cross-validation) (when set to
\code{TRUE}) or to simply fit along the sequence of values (or single
value) using \code{\link[glmnet]{glmnet}} (when set to \code{FALSE}).}

\item{prediction_bounds}{A vector of size two that provides the lower and
upper bounds for predictions. By default, the predictions are bounded
between \code{min(Y) - sd(Y) and max(Y) + sd(Y)}. Bounding ensures that
there is no extrapolation and that predictions remain bounded, which is
necessary for cross-validation selection and/or Super Learning.}

\item{lambda.min.ratio}{passed to \code{\link[glmnet]{cv.glmnet}}, controls
the ratio of largest to smallest lambda values considered}

\item{yolo}{A \code{logical} indicating whether to print one of a curated
selection of quotes from the HAL9000 computer, from the critically
acclaimed epic science-fiction film "2001: A Space Odyssey" (1968).}
}
\value{
Object of class \code{hal9001}, containing a list of basis
functions, a copy map, coefficients estimated for basis functions, and
timing results (for assessing computational efficiency).
}
\description{
Estimation procedure for HAL, the Highly Adaptive Lasso
}
\details{
The function allows users to specify the functional form/model of
hal9001 similar to in \code{\link[stats]{glm}}. The user can specify which interactions to include,
monotonicity constraints, and smoothness constraints.
The returned formula object can be fed directly into \code{fit_hal}
and the fit can be run with minimal (no) user input.

The procedure uses a custom C++ implementation to generate a design
matrix consisting of basis functions corresponding to covariates and
interactions of covariates and to remove duplicate columns of indicators.
The Lasso regression is fit to this (usually) very wide matrix using either
a custom implementation (based on \pkg{origami}) or by a call to
\code{\link[glmnet]{cv.glmnet}}.
}
\examples{
\donttest{
n <- 100
p <- 3
x <- xmat <- matrix(rnorm(n * p), n, p)
y_prob <- plogis(3 * sin(x[, 1]) + sin(x[, 2]))
y <- rbinom(n = n, size = 1, prob = y_prob)
ml_hal_fit <- fit_hal(X = x, Y = y, family = "binomial", yolo = FALSE)
preds <- predict(ml_hal_fit, new_data = x)
}

}
